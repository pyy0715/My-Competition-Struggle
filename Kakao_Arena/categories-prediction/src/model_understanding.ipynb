{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T16:15:41.193960Z",
     "start_time": "2021-03-19T16:15:40.054862Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch # 파이토치 패키지 임포트\n",
    "import torch.nn as nn # 자주 사용하는 torch.nn패키지를 별칭 nn으로 명명\n",
    "# 허깅페이스의 트랜스포머 패키지에서 BertConfig, BertModel 클래스 임포트\n",
    "from transformers import BertConfig, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T16:15:42.045506Z",
     "start_time": "2021-03-19T16:15:42.042997Z"
    }
   },
   "outputs": [],
   "source": [
    "# 전처리된 데이터가 저장된 디렉터리\n",
    "DB_PATH=f'../input/processed'\n",
    "\n",
    "# 토큰을 인덱스로 치환할 때 사용될 사전 파일이 저장된 디렉터리 \n",
    "VOCAB_DIR=os.path.join(DB_PATH, 'vocab')\n",
    "\n",
    "# 학습된 모델의 파라미터가 저장될 디렉터리\n",
    "MODEL_PATH=f'../model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T16:15:45.835111Z",
     "start_time": "2021-03-19T16:15:45.830255Z"
    }
   },
   "outputs": [],
   "source": [
    "# 미리 정의된 설정 값\n",
    "class CFG:\n",
    "    learning_rate=3.0e-4 # 러닝 레이트\n",
    "    batch_size=1024 # 배치 사이즈\n",
    "    num_workers=4 # 워커의 개수\n",
    "    print_freq=100 # 결과 출력 빈도\n",
    "    start_epoch=0 # 시작 에폭\n",
    "    num_train_epochs=10 # 학습할 에폭수\n",
    "    warmup_steps=100 # lr을 서서히 증가시킬 step 수\n",
    "    max_grad_norm=10 # 그래디언트 클리핑에 사용\n",
    "    weight_decay=0.01\n",
    "    dropout=0.2 # dropout 확률\n",
    "    hidden_size=512 # 은닉 크기\n",
    "    intermediate_size=256 # TRANSFORMER셀의 intermediate 크기\n",
    "    nlayers=2 # BERT의 층수\n",
    "    nheads=8 # BERT의 head 개수\n",
    "    seq_len=64 # 토큰의 최대 길이\n",
    "    n_b_cls = 57 + 1 # 대카테고리 개수\n",
    "    n_m_cls = 552 + 1 # 중카테고리 개수\n",
    "    n_s_cls = 3190 + 1 # 소카테고리 개수\n",
    "    n_d_cls = 404 + 1 # 세카테고리 개수\n",
    "    vocab_size = 32000 # 토큰의 유니크 인덱스 개수\n",
    "    img_feat_size = 2048 # 이미지 피처 벡터의 크기\n",
    "    type_vocab_size = 30 # 타입의 유니크 인덱스 개수\n",
    "    csv_path = os.path.join(DB_PATH, 'train.csv')\n",
    "    h5_path = os.path.join(DB_PATH, 'train_img_feat.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T16:15:46.544179Z",
     "start_time": "2021-03-19T16:15:46.530806Z"
    }
   },
   "outputs": [],
   "source": [
    "class CateClassifier(nn.Module):\n",
    "    \"\"\"상품정보를 받아서 대/중/소/세 카테고리를 예측하는 모델    \n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):        \n",
    "        \"\"\"\n",
    "        매개변수\n",
    "        cfg: hidden_size, nlayers 등 설정값을 가지고 있는 변수\n",
    "        \"\"\"\n",
    "        super(CateClassifier, self).__init__()\n",
    "        # 글로벌 설정값을 멤버 변수로 저장\n",
    "        self.cfg = cfg\n",
    "        # 버트모델의 설정값을 멤버 변수로 저장\n",
    "        self.bert_cfg = BertConfig( \n",
    "            cfg.vocab_size, # 사전 크기\n",
    "            hidden_size=cfg.hidden_size, # 히든 크기\n",
    "            num_hidden_layers=cfg.nlayers, # 레이어 층 수\n",
    "            num_attention_heads=cfg.nheads, # 어텐션 헤드의 수\n",
    "            intermediate_size=cfg.intermediate_size, # 인터미디어트 크기\n",
    "            hidden_dropout_prob=cfg.dropout, # 히든 드롭아웃 확률 값\n",
    "            attention_probs_dropout_prob=cfg.dropout, # 어텐션 드롭아웃 확률 값 \n",
    "            max_position_embeddings=cfg.seq_len, # 포지션 임베딩의 최대 길이\n",
    "            type_vocab_size=cfg.type_vocab_size, # 타입 사전 크기\n",
    "        )\n",
    "        # 텍스트 인코더로 버트모델 사용\n",
    "        self.text_encoder = BertModel(self.bert_cfg)\n",
    "        # 이미지 인코더로 선형모델 사용(대회에서 이미지가 아닌 img_feat를 제공)\n",
    "        self.img_encoder = nn.Linear(cfg.img_feat_size, cfg.hidden_size)\n",
    "                \n",
    "        # 분류기(Classifier) 생성기\n",
    "        def get_cls(target_size=1):\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(cfg.hidden_size*2, cfg.hidden_size),\n",
    "                nn.LayerNorm(cfg.hidden_size),\n",
    "                nn.Dropout(cfg.dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(cfg.hidden_size, target_size),\n",
    "            )        \n",
    "          \n",
    "        # 대 카테고리 분류기\n",
    "        self.b_cls = get_cls(cfg.n_b_cls)\n",
    "        # 중 카테고리 분류기\n",
    "        self.m_cls = get_cls(cfg.n_m_cls)\n",
    "        # 소 카테고리 분류기\n",
    "        self.s_cls = get_cls(cfg.n_s_cls)\n",
    "        # 세 카테고리 분류기\n",
    "        self.d_cls = get_cls(cfg.n_d_cls)\n",
    "    \n",
    "    def forward(self, token_ids, token_mask, token_types, img_feat, label=None):\n",
    "        \"\"\"        \n",
    "        매개변수\n",
    "        token_ids: 전처리된 상품명을 인덱스로 변환하여 token_ids를 만들었음\n",
    "        token_mask: 실제 token_ids의 개수만큼은 1, 나머지는 0으로 채움\n",
    "        token_types: ▁ 문자를 기준으로 서로 다른 타입의 토큰임을 타입 인덱스로 저장\n",
    "        img_feat: resnet50으로 인코딩된 이미지 피처\n",
    "        label: 정답 대/중/소/세 카테고리\n",
    "        \"\"\"\n",
    "\n",
    "        # 전처리된 상품명을 하나의 텍스트벡터(text_vec)로 변환\n",
    "        # 반환 튜플(시퀀스 아웃풋, 풀드(pooled) 아웃풋) 중 시퀀스 아웃풋만 사용\n",
    "        text_output = self.text_encoder(token_ids, token_mask, token_type_ids=token_types)[0] # (batch_size, sequence_length, hidden_size)\n",
    "         \n",
    "        # 시퀀스 중 첫 타임스탭의 hidden state만 사용. \n",
    "        text_vec = text_output[:, 0] # (batch_size, text_hidden_size)\n",
    "        \n",
    "        # img_feat를 텍스트벡터와 결합하기 앞서 선형변환 적용\n",
    "        img_vec = self.img_encoder(img_feat) #(batch_size, img_hidden_size)\n",
    "        \n",
    "        # 이미지벡터와 텍스트벡터를 직렬연결(concatenate)하여 결합벡터 생성\n",
    "        comb_vec = torch.cat([text_vec, img_vec], 1) # (batch_size, hidden_size*2)\n",
    "        \n",
    "        # 결합된 벡터로 대카테고리 확률분포 예측\n",
    "        b_pred = self.b_cls(comb_vec)\n",
    "        # 결합된 벡터로 중카테고리 확률분포 예측\n",
    "        m_pred = self.m_cls(comb_vec)\n",
    "        # 결합된 벡터로 소카테고리 확률분포 예측\n",
    "        s_pred = self.s_cls(comb_vec)\n",
    "        # 결합된 벡터로 세카테고리 확률분포 예측\n",
    "        d_pred = self.d_cls(comb_vec)\n",
    "        \n",
    "        # 데이터 패러럴 학습에서 GPU 메모리를 효율적으로 사용하기 위해 \n",
    "        # loss를 모델 내에서 계산함.\n",
    "        if label is not None:\n",
    "            # 손실(loss) 함수로 CrossEntropyLoss를 사용\n",
    "            # label의 값이 -1을 가지는 샘플은 loss계산에 사용 안 함\n",
    "            loss_func = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "            # label은 batch_size x 4를 (batch_size x 1) 4개로 만듦\n",
    "            b_label, m_label, s_label, d_label = label.split(1, 1)\n",
    "            # 대카테고리의 예측된 확률분포와 정답확률 분포의 차이를 손실로 반환\n",
    "            b_loss = loss_func(b_pred, b_label.view(-1))\n",
    "            # 중카테고리의 예측된 확률분포와 정답확률 분포의 차이를 손실로 반환\n",
    "            m_loss = loss_func(m_pred, m_label.view(-1))\n",
    "            # 소카테고리의 예측된 확률분포와 정답확률 분포의 차이를 손실로 반환\n",
    "            s_loss = loss_func(s_pred, s_label.view(-1))\n",
    "            # 세카테고리의 예측된 확률분포와 정답확률 분포의 차이를 손실로 반환\n",
    "            d_loss = loss_func(d_pred, d_label.view(-1))\n",
    "            # 대/중/소/세 손실의 평균을 낼 때 실제 대회 평가방법과 일치하도록 함\n",
    "            loss = (b_loss + 1.2*m_loss + 1.3*s_loss + 1.4*d_loss)/4    \n",
    "        else: # label이 없으면 loss로 0을 반환\n",
    "            loss = b_pred.new(1).fill_(0)      \n",
    "        \n",
    "        # 최종 계산된 손실과 예측된 대/중/소/세 각 확률분포를 반환\n",
    "        return loss, [b_pred, m_pred, s_pred, d_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T17:17:01.963904Z",
     "start_time": "2021-03-16T17:17:01.679542Z"
    }
   },
   "outputs": [],
   "source": [
    "cfg = CFG()\n",
    "\n",
    "model = CateClassifier(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T17:17:18.739458Z",
     "start_time": "2021-03-16T17:17:18.735393Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters:  24637551\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print('parameters: ', count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T17:17:44.364695Z",
     "start_time": "2021-03-16T17:17:44.360680Z"
    }
   },
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())    \n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params':[p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T17:20:49.422201Z",
     "start_time": "2021-03-16T17:20:49.416784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_encoder.embeddings.LayerNorm.weight\n",
      "text_encoder.embeddings.LayerNorm.bias\n",
      "text_encoder.encoder.layer.0.attention.self.query.bias\n",
      "text_encoder.encoder.layer.0.attention.self.key.bias\n",
      "text_encoder.encoder.layer.0.attention.self.value.bias\n",
      "text_encoder.encoder.layer.0.attention.output.dense.bias\n",
      "text_encoder.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "text_encoder.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "text_encoder.encoder.layer.0.intermediate.dense.bias\n",
      "text_encoder.encoder.layer.0.output.dense.bias\n",
      "text_encoder.encoder.layer.0.output.LayerNorm.weight\n",
      "text_encoder.encoder.layer.0.output.LayerNorm.bias\n",
      "text_encoder.encoder.layer.1.attention.self.query.bias\n",
      "text_encoder.encoder.layer.1.attention.self.key.bias\n",
      "text_encoder.encoder.layer.1.attention.self.value.bias\n",
      "text_encoder.encoder.layer.1.attention.output.dense.bias\n",
      "text_encoder.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "text_encoder.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "text_encoder.encoder.layer.1.intermediate.dense.bias\n",
      "text_encoder.encoder.layer.1.output.dense.bias\n",
      "text_encoder.encoder.layer.1.output.LayerNorm.weight\n",
      "text_encoder.encoder.layer.1.output.LayerNorm.bias\n",
      "text_encoder.pooler.dense.bias\n",
      "img_encoder.bias\n",
      "b_cls.0.bias\n",
      "b_cls.1.bias\n",
      "b_cls.4.bias\n",
      "m_cls.0.bias\n",
      "m_cls.1.bias\n",
      "m_cls.4.bias\n",
      "s_cls.0.bias\n",
      "s_cls.1.bias\n",
      "s_cls.4.bias\n",
      "d_cls.0.bias\n",
      "d_cls.1.bias\n",
      "d_cls.4.bias\n"
     ]
    }
   ],
   "source": [
    "for n,p in param_optimizer:\n",
    "        if any(nd in n for nd in no_decay):\n",
    "            print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T16:41:24.494424Z",
     "start_time": "2021-03-19T16:41:24.487981Z"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.rand(32, 10)\n",
    "b = torch.rand(32, 20)\n",
    "c = torch.rand(32, 30)\n",
    "d = [a,b,c]\n",
    "\n",
    "pred_list = []\n",
    "pred_list.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T16:41:34.814162Z",
     "start_time": "2021-03-19T16:41:34.811435Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_list.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T16:41:48.518366Z",
     "start_time": "2021-03-19T16:41:48.515211Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T16:42:06.218324Z",
     "start_time": "2021-03-19T16:42:06.214555Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_list[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T16:06:54.762829Z",
     "start_time": "2021-03-16T16:06:39.012265Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/home/yyeon/KeepGo/My-Competition-Struggle/Kakao_Arena/categories-prediction/input/processed/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T16:06:56.104510Z",
     "start_time": "2021-03-16T16:06:56.080696Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>tokens</th>\n",
       "      <th>bcateid</th>\n",
       "      <th>mcateid</th>\n",
       "      <th>scateid</th>\n",
       "      <th>dcateid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O4486751463</td>\n",
       "      <td>▁직소퍼즐 ▁1000 조각 ▁바다 거북 의 ▁여행 ▁pl 12 75</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P3307178849</td>\n",
       "      <td>▁모리케이스 ▁아이폰 6 s ▁6 s ▁tree ▁farm 101 ▁다이어리케이스 ...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R4424255515</td>\n",
       "      <td>▁크리비아 ▁기모 ▁3 부 ▁속바지 ▁gl g 43 14 p</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F3334315393</td>\n",
       "      <td>▁하프클럽 ▁잭앤질 ▁남성 ▁솔리드 ▁절개라인 ▁포인트 ▁포켓 ▁팬츠 ▁311 33...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N731678492</td>\n",
       "      <td>▁코드 프리 혈 당 시험 지 50 매 ▁코드 프리 시험 지 ▁최 장 유 효 기간</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pid                                             tokens  bcateid  \\\n",
       "0  O4486751463              ▁직소퍼즐 ▁1000 조각 ▁바다 거북 의 ▁여행 ▁pl 12 75        1   \n",
       "1  P3307178849  ▁모리케이스 ▁아이폰 6 s ▁6 s ▁tree ▁farm 101 ▁다이어리케이스 ...        3   \n",
       "2  R4424255515                  ▁크리비아 ▁기모 ▁3 부 ▁속바지 ▁gl g 43 14 p        5   \n",
       "3  F3334315393  ▁하프클럽 ▁잭앤질 ▁남성 ▁솔리드 ▁절개라인 ▁포인트 ▁포켓 ▁팬츠 ▁311 33...        7   \n",
       "4   N731678492       ▁코드 프리 혈 당 시험 지 50 매 ▁코드 프리 시험 지 ▁최 장 유 효 기간       10   \n",
       "\n",
       "   mcateid  scateid  dcateid  \n",
       "0        1        2       -1  \n",
       "1        3        4       -1  \n",
       "2        5        6       -1  \n",
       "3        7        8       -1  \n",
       "4        9       11       -1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T16:55:21.413463Z",
     "start_time": "2021-03-16T16:54:58.207414Z"
    }
   },
   "outputs": [],
   "source": [
    "df['unique_cateid'] = (df['bcateid'].astype('str') +\n",
    "                       df['mcateid'].astype('str') + \n",
    "                       df['scateid'].astype('str') + df['dcateid'].astype('str')).astype('category')\n",
    "df['unique_cateid'] = df['unique_cateid'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T16:55:47.185389Z",
     "start_time": "2021-03-16T16:55:47.176687Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>tokens</th>\n",
       "      <th>bcateid</th>\n",
       "      <th>mcateid</th>\n",
       "      <th>scateid</th>\n",
       "      <th>dcateid</th>\n",
       "      <th>unique_cateid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O4486751463</td>\n",
       "      <td>▁직소퍼즐 ▁1000 조각 ▁바다 거북 의 ▁여행 ▁pl 12 75</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P3307178849</td>\n",
       "      <td>▁모리케이스 ▁아이폰 6 s ▁6 s ▁tree ▁farm 101 ▁다이어리케이스 ...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>2295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R4424255515</td>\n",
       "      <td>▁크리비아 ▁기모 ▁3 부 ▁속바지 ▁gl g 43 14 p</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>3720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F3334315393</td>\n",
       "      <td>▁하프클럽 ▁잭앤질 ▁남성 ▁솔리드 ▁절개라인 ▁포인트 ▁포켓 ▁팬츠 ▁311 33...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>3944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N731678492</td>\n",
       "      <td>▁코드 프리 혈 당 시험 지 50 매 ▁코드 프리 시험 지 ▁최 장 유 효 기간</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>-1</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pid                                             tokens  bcateid  \\\n",
       "0  O4486751463              ▁직소퍼즐 ▁1000 조각 ▁바다 거북 의 ▁여행 ▁pl 12 75        1   \n",
       "1  P3307178849  ▁모리케이스 ▁아이폰 6 s ▁6 s ▁tree ▁farm 101 ▁다이어리케이스 ...        3   \n",
       "2  R4424255515                  ▁크리비아 ▁기모 ▁3 부 ▁속바지 ▁gl g 43 14 p        5   \n",
       "3  F3334315393  ▁하프클럽 ▁잭앤질 ▁남성 ▁솔리드 ▁절개라인 ▁포인트 ▁포켓 ▁팬츠 ▁311 33...        7   \n",
       "4   N731678492       ▁코드 프리 혈 당 시험 지 50 매 ▁코드 프리 시험 지 ▁최 장 유 효 기간       10   \n",
       "\n",
       "   mcateid  scateid  dcateid  unique_cateid  \n",
       "0        1        2       -1            109  \n",
       "1        3        4       -1           2295  \n",
       "2        5        6       -1           3720  \n",
       "3        7        8       -1           3944  \n",
       "4        9       11       -1             49  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T16:56:54.092013Z",
     "start_time": "2021-03-16T16:56:54.010034Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4214"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['unique_cateid'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T17:05:09.226762Z",
     "start_time": "2021-03-16T17:05:07.255452Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "folds = KFold(n_splits =5, shuffle=True)\n",
    "\n",
    "train_idx, valid_idx = list(folds.split(df.values))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T17:06:52.562631Z",
     "start_time": "2021-03-16T17:06:52.558261Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7999999508286479"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_idx) / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T16:15:55.093882Z",
     "start_time": "2021-03-19T16:15:55.050231Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch # 파이토치 패키지 임포트\n",
    "from torch.utils.data import Dataset # Dataset 클래스 임포트\n",
    "import h5py # h5py 패키지 임포트\n",
    "import re # 정규식표현식 모듈 임포트 \n",
    "\n",
    "class CateDataset(Dataset):\n",
    "    \"\"\"데이터셋에서 학습에 필요한 형태로 변환된 샘플 하나를 반환\n",
    "    \"\"\"\n",
    "    def __init__(self, df_data, img_h5_path, token2id, tokens_max_len=64, type_vocab_size=30):\n",
    "        \"\"\"        \n",
    "        매개변수\n",
    "        df_data: 상품타이틀, 카테고리 등의 정보를 가지는 데이터프레임\n",
    "        img_h5_path: img_feat가 저장돼 있는 h5 파일의 경로\n",
    "        token2id: token을 token_id로 변환하기 위한 맵핑 정보를 가진 딕셔너리\n",
    "        tokens_max_len: tokens의 최대 길이. 상품명의 tokens가 이 이상이면 잘라서 버림\n",
    "        type_vocab_size: 타입 사전의 크기\n",
    "        \"\"\"        \n",
    "        self.tokens = df_data['tokens'].values # 전처리된 상품명\n",
    "        self.img_indices = df_data['img_idx'].values # h5의 이미지 인덱스\n",
    "        self.img_h5_path = img_h5_path \n",
    "        self.tokens_max_len = tokens_max_len        \n",
    "        self.labels = df_data[['bcateid', 'mcateid', 'scateid', 'dcateid']].values\n",
    "        self.token2id = token2id \n",
    "        self.p = re.compile('▁[^▁]+') # ▁기호를 기준으로 나누기 위한 컴파일된 정규식\n",
    "        self.type_vocab_size = type_vocab_size\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        데이터셋에서 idx에 대응되는 샘플을 변환하여 반환        \n",
    "        \"\"\"\n",
    "        if idx >= len(self):\n",
    "            raise StopIteration\n",
    "        \n",
    "        # idx에 해당하는 상품명 가져오기. 상품명은 문자열로 저장돼 있음\n",
    "        tokens = self.tokens[idx]\n",
    "        if not isinstance(tokens, str):\n",
    "            tokens = ''\n",
    "        \n",
    "        # 상품명을 ▁기호를 기준으로 분리하여 파이썬 리스트로 저장\n",
    "        # \"▁직소퍼즐 ▁1000 조각 ▁바다 거북 의 ▁여행 ▁pl 12 75\" =>\n",
    "        # [\"▁직소퍼즐\", \"▁1000 조각\", \"▁바다 거북 의\", \"▁여행\", \"▁pl 12 75\"]\n",
    "        tokens = self.p.findall(tokens)\n",
    "        \n",
    "        # ▁ 기호 별 토큰타입 인덱스 부여\n",
    "        # [\"▁직소퍼즐\", \"▁1000 조각\", \"▁바다 거북 의\", \"▁여행\", \"▁pl 12 75\"] =>\n",
    "        # [     0     ,     1    1  ,    2     2  2 ,     3   ,   4  4   4 ]\n",
    "        token_types = [type_id for type_id, word in enumerate(tokens) for _ in word.split()]       \n",
    "        tokens = \" \".join(tokens) # ▁기호로 분리되기 전의 원래의 tokens으로 되돌림\n",
    "\n",
    "        # 토큰을 토큰에 대응되는 인덱스로 변환\n",
    "        # \"▁직소퍼즐 ▁1000 조각 ▁바다 거북 의 ▁여행 ▁pl 12 75\" =>\n",
    "        # [2291, 784, 2179, 3540, 17334, 30827, 1114, 282, 163, 444]\n",
    "        # \"▁직소퍼즐\" => 2291\n",
    "        # \"▁1000\" => 784\n",
    "        # \"조각\" => 2179\n",
    "        # ...\n",
    "        token_ids = [self.token2id[tok] if tok in self.token2id else 0 for tok in tokens.split()]\n",
    "        \n",
    "        # token_ids의 길이가 max_len보다 길면 잘라서 버림\n",
    "        if len(token_ids) > self.tokens_max_len:\n",
    "            token_ids = token_ids[:self.tokens_max_len]      \n",
    "            token_types = token_types[:self.tokens_max_len]\n",
    "        \n",
    "        # token_ids의 길이가 max_len보다 짧으면 짧은만큼 PAD값 0 값으로 채워넣음\n",
    "        # token_ids 중 값이 있는 곳은 1, 그 외는 0으로 채운 token_mask 생성\n",
    "        token_mask = [1] * len(token_ids)\n",
    "        token_pad = [0] * (self.tokens_max_len - len(token_ids))\n",
    "        token_ids += token_pad\n",
    "        token_mask += token_pad\n",
    "        token_types += token_pad # max_len 보다 짧은만큼 PAD 추가\n",
    "\n",
    "        # h5파일에서 이미지 인덱스에 해당하는 img_feat를 가져옴\n",
    "        # 파이토치의 데이터로더에 의해 동시 h5파일에 동시접근이 발생해도\n",
    "        # 안정적으로 img_feat를 가져오려면 아래처럼 매번 h5py.File 호출필요\n",
    "        with h5py.File(self.img_h5_path, 'r') as img_feats:\n",
    "            img_feat = img_feats['img_feat'][self.img_indices[idx]]\n",
    "        \n",
    "        # 넘파이(numpy)나 파이썬 자료형을 파이토치의 자료형으로 변환\n",
    "        token_ids = torch.LongTensor(token_ids)\n",
    "        token_mask = torch.LongTensor(token_mask)\n",
    "        token_types = torch.LongTensor(token_types)\n",
    "        \n",
    "        # token_types의 타입 인덱스의 숫자 크기가 type_vocab_size 보다 작도록 바꿈\n",
    "        token_types[token_types >= self.type_vocab_size] = self.type_vocab_size-1 \n",
    "        img_feat = torch.FloatTensor(img_feat)\n",
    "        \n",
    "        # 대/중/소/세 라벨 준비\n",
    "        label = self.labels[idx]\n",
    "        label = torch.LongTensor(label)\n",
    "        \n",
    "        # 크게 3가지 텍스트 입력, 이미지 입력, 라벨을 반환한다.\n",
    "        return token_ids, token_mask, token_types, img_feat, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "          tokens의 개수를 반환한다. 즉, 상품명 문장의 개수를 반환한다.\n",
    "        \"\"\"\n",
    "        return len(self.tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T16:15:53.754052Z",
     "start_time": "2021-03-16T16:15:53.751714Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens = [\"▁직소퍼즐\", \"▁1000 조각\", \"▁바다 거북 의\", \"▁여행\", \"▁pl 12 75\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T16:15:58.291430Z",
     "start_time": "2021-03-16T16:15:58.287598Z"
    }
   },
   "outputs": [],
   "source": [
    "token_types = [type_id for type_id, word in enumerate(tokens) for _ in word.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T16:16:01.387459Z",
     "start_time": "2021-03-16T16:16:01.383549Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 2, 2, 2, 3, 4, 4, 4]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T16:16:43.381843Z",
     "start_time": "2021-03-19T16:16:43.373128Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T16:18:43.835840Z",
     "start_time": "2021-03-19T16:18:43.461740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint '../model/b1024_h512_d0.2_l2_hd8_ep2_s7_fold0.pt'\n",
      "=> loaded checkpoint '../model/b1024_h512_d0.2_l2_hd8_ep2_s7_fold0.pt' (epoch 3)\n"
     ]
    }
   ],
   "source": [
    "model_list = []\n",
    "# args.model_dir에 있는 확장자 .pt를 가지는 모든 모델 파일의 경로를 읽음\n",
    "model_path_list = glob.glob(os.path.join(MODEL_PATH, '*.pt'))\n",
    "# 모델 경로 개수만큼 모델을 생성하여 파이썬 리스트에 추가함\n",
    "for model_path in model_path_list:\n",
    "    model = CateClassifier(CFG)\n",
    "    if model_path != \"\":\n",
    "        print(\"=> loading checkpoint '{}'\".format(model_path))\n",
    "        checkpoint = torch.load(model_path)        \n",
    "        state_dict = checkpoint['state_dict']                \n",
    "        model.load_state_dict(state_dict, strict=True)  \n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\".format(model_path, checkpoint['epoch']))\n",
    "    model.cuda()\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    if n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    model_list.append(model)\n",
    "    \n",
    "if len(model_list) == 0:\n",
    "    print('Please check the model directory.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-19T16:19:22.854807Z",
     "start_time": "2021-03-19T16:19:22.850056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CateClassifier(\n",
       "   (text_encoder): BertModel(\n",
       "     (embeddings): BertEmbeddings(\n",
       "       (word_embeddings): Embedding(32000, 512, padding_idx=0)\n",
       "       (position_embeddings): Embedding(64, 512)\n",
       "       (token_type_embeddings): Embedding(30, 512)\n",
       "       (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.2, inplace=False)\n",
       "     )\n",
       "     (encoder): BertEncoder(\n",
       "       (layer): ModuleList(\n",
       "         (0): BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "               (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "               (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "               (dropout): Dropout(p=0.2, inplace=False)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "               (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.2, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=512, out_features=256, bias=True)\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=256, out_features=512, bias=True)\n",
       "             (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.2, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (1): BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "               (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "               (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "               (dropout): Dropout(p=0.2, inplace=False)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "               (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.2, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=512, out_features=256, bias=True)\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=256, out_features=512, bias=True)\n",
       "             (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.2, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (pooler): BertPooler(\n",
       "       (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "       (activation): Tanh()\n",
       "     )\n",
       "   )\n",
       "   (img_encoder): Linear(in_features=2048, out_features=512, bias=True)\n",
       "   (b_cls): Sequential(\n",
       "     (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "     (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "     (2): Dropout(p=0.2, inplace=False)\n",
       "     (3): ReLU()\n",
       "     (4): Linear(in_features=512, out_features=58, bias=True)\n",
       "   )\n",
       "   (m_cls): Sequential(\n",
       "     (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "     (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "     (2): Dropout(p=0.2, inplace=False)\n",
       "     (3): ReLU()\n",
       "     (4): Linear(in_features=512, out_features=553, bias=True)\n",
       "   )\n",
       "   (s_cls): Sequential(\n",
       "     (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "     (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "     (2): Dropout(p=0.2, inplace=False)\n",
       "     (3): ReLU()\n",
       "     (4): Linear(in_features=512, out_features=3191, bias=True)\n",
       "   )\n",
       "   (d_cls): Sequential(\n",
       "     (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "     (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "     (2): Dropout(p=0.2, inplace=False)\n",
       "     (3): ReLU()\n",
       "     (4): Linear(in_features=512, out_features=405, bias=True)\n",
       "   )\n",
       " )]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_Server",
   "language": "python",
   "name": "dl_server"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
